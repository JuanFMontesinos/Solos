---
layout: archive
permalink: /
title: "Solos: A Dataset for Audio-Visual Music Source Separation and Localization "
excerpt: "Audiovisual BSS Dataset"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
<p>Solos is a YouTube gathered dataset containing music excerpts of players playing different instruments for auditions.<br>
This dataset is complementary to other datasets of this nature such us MUSIC and MUSICes.</p>
<p> </p>
<p>Solos provide frame-wise human skeletons for the soloist computed using
<a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose/">OpenPose</a>. For each frame we provide meaningful joints, namely, the upperbody joints + hand detection.</p>

<iframe width="1208" height="680" src="https://www.youtube.com/embed/17Kjw976myc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><p> </p>
In the next video we can see skeletons provided for both, the interpolated and non-interpolated cases. Skeletons have been colored according to 
the network confidence. The more blue the more confident the network is. The more red, the less confidence the network is. Green is used when 
the network fails to predict a joint (miss-prediction). 

<p> </p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/pnc0tPYzW5U?start=20" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<p> </p>
As we can observe, once the network fails to predict a joint, the joint position goes to (0,0), the top left of the image. This harms DL models performance.
That is why we do provide an interpolated version as described in the work. 
We also can observe there is a flickering effect in the joint position due to the fact they are computed framewise. In order to address this problem 
we recommend to use a savgol filter.
<p> </p>
<h1>Categories</h1>  
Solos contains the same categories than
<a href="http://www2.ece.rochester.edu/projects/air/projects/URMP.html">URMP Dataset</a>
<img src="images/instruments.png"
     alt="Categories"
     title="Solos categories.">

This is intended in order to be able to use URMP as test set.  
<p> </p>

<h1> Dataset Statistics</h1>
<p>In the following table we show the statistics of the dataset: </p>
  <table>
    <tr>
        <td>Category</td>
        <td># Recordings</td>
        <td>Mean duration</td>
        <td>Median resolution</td>
    </tr>
    <tr>
        <td>Violin</td>
        <td>66</td>
        <td>6:16</td>
        <td>1080x720</td>
    </tr>
    <tr>
        <td>Viola</td>
        <td>55</td>
        <td>5:31</td>
        <td>1280x720</td>
    </tr>
    <tr>
        <td>Cello</td>
        <td>134</td>
        <td>7:21</td>
        <td>640x480</td>
    </tr>
    <tr>
        <td>DoubleBass</td>
        <td>58</td>
        <td>8:53</td>
        <td>1280x720</td>
    </tr>
    <tr>
        <td>Flute</td>
        <td>48</td>
        <td>4:00</td>
        <td>640x360</td>
    </tr>
    <tr>
        <td>Oboe</td>
        <td>53</td>
        <td>5:45</td>
        <td>1280x720</td>
    </tr>
    <tr>
        <td>Clarinet</td>
        <td>49</td>
        <td>3:23</td>
        <td>640x360</td>
    </tr>
    <tr>
        <td>Bassoon</td>
        <td>56</td>
        <td>5:08</td>
        <td>1280x720</td>
    </tr>
    <tr>
        <td>Saxophone</td>
        <td>45</td>
        <td>2:42</td>
        <td>1280x720</td>
    </tr>
    <tr>
        <td>Trumpet</td>
        <td>50</td>
        <td>1:14</td>
        <td>640x360</td>
    </tr>
    <tr>
        <td>Horn</td>
        <td>50</td>
        <td>5:11</td>
        <td>1280x720</td>
    </tr>
    <tr>
        <td>Trombone</td>
        <td>50</td>
        <td>5:03</td>
        <td>1280x720</td>
    </tr>
    <tr>
        <td>Tuba</td>
        <td>41</td>
        <td>2:49</td>
        <td>640x360</td>
    </tr>
    <tr>
        <td><b>TOTAL</b></td>
        <td><b>755</b></td>
        <td><b>5:16</b></td>
        <td><b>854x480</b></td>
    </tr>
</table>
<h1>Results</h1>  
<p> In the following Table we show a comparison between Sound of Pixels trained in MUSIC,
 trained in SOLOS, trained in MUSIC and fine tuned in solos and a Multi-Head UNet trained on Solos. </p>
<table>
    <tr>
        <td></td>
        <td>SDR $\uparrow$</td>
        <td>SIR $\uparrow$</td>
        <td>SAR $\uparrow$</td>
    </tr>
    <tr>
        <td>SoP</td>
        <td>$-3.76\pm4.00$</td>
        <td>$-1.45\pm4.68$</td>
        <td>$7.56\pm3.13$</td>
    </tr>
    <tr>
        <td>SoP-Solos</td>
        <td>$-2.98\pm5.07$</td>
        <td>$0.46\pm6.76$</td>
        <td>$6.37\pm2.94$</td>
    </tr>
    <tr>
        <td>SoP-ft</td>
        <td>$-2.57\pm4.99$</td>
        <td>$0.47\pm6.43$</td>
        <td>$6.89\pm2.48$</td>
    </tr>
    <tr>
        <td>MHU-Net</td>
        <td>$ -0.56\pm5.96 $</td>
        <td>$ 1.04\pm7.24 $</td>
        <td>$ 10.37\pm3.48 $</td>
    </tr>
</table>
<p> </p>
<h1>Citation</h1>
To appear in MMSP 2020
<pre>
@article{montesinos2020solos,
    title={Solos: A Dataset for Audio-Visual Music Analysis},
    author={Juan F. Montesinos and Olga Slizovskaia and Gloria Haro},
    year={2020},
    journal={arXiv preprint arXiv:2006.07931}
    }
</pre>
